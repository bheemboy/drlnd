{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "Below, we will learn to implement and train a policy to play atari-pong, using only the pixels as input. We will use convolutional neural nets, multiprocessing, and pytorch to implement and train our policy. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: JSAnimation in c:\\miniconda3\\envs\\drlnd\\lib\\site-packages (0.1)\n",
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# install package for displaying animation\n",
    "!pip install JSAnimation\n",
    "\n",
    "# custom utilies for displaying animation, collecting rollouts and more\n",
    "import pong_utils\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = pong_utils.device\n",
    "print(\"using device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# render ai gym environment\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life\n",
    "# the actions are hard-coded in pong_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "_, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(pong_utils.preprocess_single(frame), cmap='Greys')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# set up a convolutional neural net\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "        \n",
    "        # 80x80 to outputsize x outputsize\n",
    "        # outputsize = (inputsize - kernel_size + stride)/stride \n",
    "        # (round up if not an integer)\n",
    "\n",
    "        # output = 20x20 here\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=4, stride=4)\n",
    "        self.size=1*20*20\n",
    "        \n",
    "        # 1 fully connected layer\n",
    "        self.fc = nn.Linear(self.size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "    \n",
    "        x = F.relu(self.conv(x))\n",
    "        # flatten the tensor\n",
    "        x = x.view(-1,self.size)\n",
    "        return self.sig(self.fc(x))\n",
    "\n",
    "\n",
    "# run your own policy!\n",
    "# policy=Policy().to(device)\n",
    "policy=pong_utils.Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game visualization\n",
    "pong_utils contain a play function given the environment and a policy. An optional preprocess function can be supplied. Here we define a function that plays a game and shows learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_utils.play(env, policy, time=200) \n",
    "# try to add the option \"preprocess=pong_utils.preprocess_single\"\n",
    "# to see what the agent sees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. \n",
    "\n",
    "## Exercise 2: write your own function for training\n",
    "(what I call scalar function is the same as policy_loss up to a negative sign)\n",
    "\n",
    "### PPO\n",
    "Later on, you'll implement the PPO algorithm as well, and the scalar function is given by\n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$\n",
    "\n",
    "the ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "\n",
    "    ########\n",
    "    ## \n",
    "    ## WRITE YOUR OWN CODE HERE\n",
    "    ##\n",
    "    ########\n",
    "    \n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = pong_utils.states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == pong_utils.RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # prevents policy to become exactly 0 or 1 helps exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    return torch.mean(beta*entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: progressbar in c:\\miniconda3\\envs\\drlnd\\lib\\site-packages (2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |#                                          | ETA:  1:42:29\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, score: -15.375000\n",
      "[-16. -16. -13. -16. -15. -16. -15. -16.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   8% |###                                        | ETA:  1:34:18\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40, score: -14.125000\n",
      "[-15. -12. -15. -14. -16. -14. -11. -16.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                      | ETA:  1:26:59\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 60, score: -13.500000\n",
      "[-15. -13. -13. -16. -12. -15. -12. -12.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  16% |######                                     | ETA:  1:21:41\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 80, score: -13.625000\n",
      "[-15. -11. -13. -14. -15. -12. -17. -12.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                   | ETA:  1:17:18\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, score: -14.500000\n",
      "[-16. -14. -17. -16. -16. -15. -14.  -8.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  24% |##########                                 | ETA:  1:13:01\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 120, score: -12.500000\n",
      "[-16. -15. -14.  -8. -13. -16. -10.  -8.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  28% |############                               | ETA:  1:09:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 140, score: -11.875000\n",
      "[-12. -13. -13. -11. -12. -12. -12. -10.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  32% |#############                              | ETA:  1:05:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 160, score: -10.500000\n",
      "[ -8. -14. -11. -13.  -8. -11.  -4. -15.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  36% |###############                            | ETA:  1:01:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 180, score: -11.125000\n",
      "[-12. -11. -12.  -8. -13. -14.  -9. -10.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  40% |#################                          | ETA:  0:57:13\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200, score: -10.500000\n",
      "[-11. -12.  -9. -11. -14. -10. -10.  -7.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  44% |##################                         | ETA:  0:53:19\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 220, score: -7.250000\n",
      "[ -8.  -4.  -7.  -7.  -7. -10.  -8.  -7.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  48% |####################                       | ETA:  0:49:26\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 240, score: -6.875000\n",
      "[-3. -9. -8. -6. -6. -7. -9. -7.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  52% |######################                     | ETA:  0:45:34\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 260, score: -5.375000\n",
      "[ -1. -10.  -5.  -3.  -7.  -5.  -8.  -4.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  56% |########################                   | ETA:  0:41:41\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 280, score: -3.625000\n",
      "[  0.  -5. -10.   0.  -1.  -5.  -4.  -4.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  60% |#########################                  | ETA:  0:37:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300, score: -2.625000\n",
      "[-6. -2.  0. -2. -2. -1. -6. -2.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  64% |###########################                | ETA:  0:34:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 320, score: -2.125000\n",
      "[ 0. -5. -1. -3.  0. -7.  0. -1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  68% |#############################              | ETA:  0:30:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 340, score: -0.750000\n",
      "[ 0. -4. -1. -1.  1.  0. -1.  0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  72% |##############################             | ETA:  0:26:25\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 360, score: -0.500000\n",
      "[ 2. -7. -2.  3.  3. -3.  0.  0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  76% |################################           | ETA:  0:22:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 380, score: -0.250000\n",
      "[-1. -2.  3. -2. -2.  1.  1.  0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  80% |##################################         | ETA:  0:18:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 400, score: 0.375000\n",
      "[ 4.  0.  3. -2.  1. -4.  1.  0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  84% |####################################       | ETA:  0:15:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 420, score: 0.750000\n",
      "[ 4.  1. -3. -2.  4.  3.  1. -2.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  88% |#####################################      | ETA:  0:11:17\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 440, score: 1.625000\n",
      "[ 4. -2. -2.  1.  3.  3.  1.  5.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  92% |#######################################    | ETA:  0:07:31\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 460, score: 3.500000\n",
      "[5. 3. 2. 0. 5. 5. 3. 5.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  96% |#########################################  | ETA:  0:03:45\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 480, score: 3.250000\n",
      "[ 5.  5. -1.  5.  1.  1.  5.  5.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop: 100% |###########################################| Time: 1:33:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500, score: 3.125000\n",
      "[ 5. -2.  5.  5.  5.  3. -1.  5.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "# keep track of how long training takes\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 500\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 320\n",
    "SGD_epoch = 4\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        # L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        L = -pong_utils.clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                                          epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_utils.play(env, policy, time=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "torch.save(policy, 'PPO.policy')\n",
    "\n",
    "# load policy if needed\n",
    "# policy = torch.load('PPO.policy')\n",
    "\n",
    "# try and test out the solution \n",
    "# make sure GPU is enabled, otherwise loading will fail\n",
    "# (the PPO verion can win more often than not)!\n",
    "#\n",
    "# policy_solution = torch.load('PPO_solution.policy')\n",
    "# pong_utils.play(env, policy_solution, time=2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
